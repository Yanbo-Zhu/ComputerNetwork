# 1 传输控制协议TCP简介

•面向连接的、可靠的、基于字节流的传输层通信协议；
•将应用的数据分割成报文段并发送给目标节点的TCP层；
•数据包都有序号Sequence，对方收到则发送ACK确认，未收到则重传；
•使用检验和来检验数据传输过程中是否有误；

- **_Ende-zu-Ende-Transportprotokoll_** im Internet
- **_Zuverlässig_**, **_verbindungsorientiert_**, **_paketvermittelt_**
- **_Bidirektional_**, d.h. Übertragung in beide Richtungen
- **_Flusskontrolle_** vermeidet Überlastung des Empfängers
- **_Staukontrolle_** drosselt die Übertragung bei (angeblicher) Überlastung des Netzes
- **_Fehlerkontrolle_** ermöglicht Wiederübertragung bei fehlerhaften oder verlorenen Segmenten 
- Sender: Unterteilt den eingehenden Datenstrom auf mehrere Segmente und übergibt sie der Internetschicht
- Empfänger: Setzt die eingehenden Segmente in der ursprünglichen Reihenfolge zu einem Ausgabestrom zusammen und übergibt ihn der Anwendungsschicht


- point-to-point:
    - one sender, one receiver
- reliable, in-order byte steam:
    - no “message boundaries"
- full duplex data:
    - bi-directional data flow in same connection
- MSS: maximum segment size
- cumulative ACKs
- pipelining:
    - TCP congestion and flow control set window size
- connection-oriented:
    - handshaking (exchange of control messages) initializes sender, receiver state before data exchange 
- flow controlled:
    - sender will not overwhelm receiver

# 2 TCP和UDP

TCP ist connection oriented reliable transport 
UDP ist connectless transport 


![](../60_01_Intro/image/Pasted%20image%2020241021072630.png)



# 3 Error Control, Congestion Control, and Flow Control

- Objective: management of connections while they are in use
- Key issues:
    - Error control – ensuring that data is delivered with desired level of reliability, e.g. all of the data is delivered without any errors
    - Congestion control – avoid overload situation within the network
    - Flow control – keeping a fast transmitter from overrunning a slow receiver
Note: These issues are also addressed in data link layer (not covered in this lecture)


- Foundation for `error control `is a sliding window with checksums and retransmissions
- `Flow control` manages buffering at sender/receiver
    - Issue is that data goes to/from the network and applications at different times
    - Window tells sender available buffering at receiver
    - Makes a variable-size sliding window
- `Congestion control` limits transmissions from sender based on network’s carrying capacity



# 4 Error Control (checksum and retransmissions)

- Difference in function: e.g., error detection
    - Link layer (LL) checksum protects a frame while it crosses a single link
    - Transport layer checksum protects a segment while it crosses an entire network path
        - Note: packets may get corrupted inside a router
    - End-to-end argument: transport layer check that runs end-to-end is essential for correctness => link layer checks are not essential but improve performance (a corrupted packet will not be sent along the entire path  unnecessarily)

![](image/Pasted%20image%2020241208192900.png)


## 4.1 Difference in degree: e.g., retransmissions and the sliding window protocol

- Most wireless links (except satellite links) can have only a single frame outstanding from the sender at a time, i.e. bandwidth-delay product is too small to store even a whole frame => small window size is sufficient for good performance
- E.g., 802.11 uses a stop-and-wait protocol, i.e., (re-) transmitting each frame and waiting for the ACKed before moving to next frame
    - Larger window size would add complexity without improving performance
- For wired/optical fiber links (e.g., (switched) Ethernet or ISP backbones) the error-rate is low enough that link-layer retransmissions can be omitted => end-to-end retransmissions will repair residual frame loss

But many transport layer (e.g., TCP) connections have a large bandwidth-delay product that is much larger than a single segment
Example:
- Connection sending data across the U.S. at 1 Mbps with a round-trip time of 100 ms
- Here 200 Kbit of data will be stored at receiver in the time it takes to send a segment and receive the ACK
- For such situations, a large sliding window must be used! 
- Note: stop-and-wait will kill performance, e.g. in our example it would limit performance to just 5 segments/sec no matter how fast the network really is!



# 5 round trip time, timeout

Q: how to set TCP timeout value?
- longer than RTT, but RTT varies!
- too short: premature timeout, unnecessary retransmissions
- too long: slow reaction to segment loss

Q: how to estimate RTT?
- SampleRTT:measured time from segment transmission until ACK receipt
    - ignore retransmissions
- SampleRTT will vary, want estimated RTT “smoother”
    - average several recent measurements, not just current SampleRTT

![](image/Pasted%20image%2020241208180348.png)


![](image/Pasted%20image%2020241208180355.png)

# 6 TCP Sender (simplified)

event: data received from application
- create segment with seq 
- seq # is byte-stream number of first data byte in segment
- start timer if not already running
    - think of timer as for oldest unACKed segment
    - expiration interval: TimeOutInterval

event: timeout
- retransmit segment that caused timeout
- restart timer

event: ACK received
- if ACK acknowledges previously unACKed segments
- update what is known to be ACKed
- start timer if there are still unACKed segments

![](image/Pasted%20image%2020241208183019.png)


## 6.1 TCP: retransmission scenarios

![](image/Pasted%20image%2020241208180626.png)


![](image/Pasted%20image%2020241208180632.png)



![](image/Pasted%20image%2020241208180715.png)


# 7 Flow Control 

brief preview from data link layer

![](image/Pasted%20image%2020241207120429.png)

1. Frame carries an error-detecting code (e.g., CRC) to check if information was correctly received
2. Frame carries a sequence number to identify itself and is retransmitted by the sender until it receives an ACK of successful receipt from the receiver => ARQ (Automatic Repeat reQuest)
3. There is a maximum number of frames M that the sender will allow to be outstanding at any time, pausing if the receiver is not acknowledging frames quickly enough:
    1. If M=1: protocol is called stop-and-wait.
    2. M>1 enables pipelining and improved performance on long, fast links
4. Sliding window protocol combines these features + used to support bidirectional data transfer


## 7.1 flow control protocols
This description relates to **flow control protocols** used in computer networks for reliable data transmission. Here's a breakdown of the key points:
1. **Maximum Number of Outstanding Frames (M):**
    - This is the maximum number of data frames a sender can transmit without waiting for an acknowledgment (ACK) from the receiver.
    - It determines the level of **pipelining**, which allows for more efficient use of the communication channel.

--- 

Protocols Based on `M`:
Stop-and-Wait Protocol (M = 1):
- The sender transmits one frame and waits for an acknowledgment before sending the next frame.
- **Advantages:**
    - Simple to implement.
    - Works well on low-speed or low-latency networks.
- **Disadvantages:**
    - Inefficient on high-latency or high-bandwidth links because the sender remains idle while waiting for an acknowledgment.


Sliding Window Protocol (M > 1):
- The sender can transmit up to `M` frames without waiting for an acknowledgment.
- **Pipelining:** Allows multiple frames to be "in transit" simultaneously, improving performance by utilizing the bandwidth-delay product of the link.
- The receiver acknowledges frames and informs the sender of the next expected frame, enabling the sender to slide the "window" forward.
- **Advantages:**
    - Significantly higher throughput on high-latency or high-bandwidth links.
- **Disadvantages:**
    - More complex to implement than stop-and-wait.
    - Requires mechanisms to handle lost or out-of-order frames.


Key Concepts:
1. **Acknowledgment (ACK):**
    - A signal sent by the receiver to confirm successful receipt of a frame.
2. **Flow Control:**
    - Mechanism to prevent the sender from overwhelming the receiver (e.g., by sending too many frames at once).
3. **Retransmission:**
    - If frames or ACKs are lost, the sender may retransmit the affected frames based on timers or duplicate ACKs.

Use Cases:
- **Stop-and-Wait:** Suitable for short, low-latency links like local area networks (LANs).
- **Sliding Window:** Essential for long-distance, high-speed networks like satellite communications or wide area networks (WANs).



## 7.2 Sliding Window

- Allows continuous transmission of packets (called frames in data link)
- Continuous transmission means that the sender is allowed to send continuously without waiting for every packet to be acknowledged

![](image/Pasted%20image%2020241207121250.png)


# 8 TCP flow control (Buffering)

- As transport protocols generally use larger sliding windows we have to look at the issue of buffering data more carefully
    - … a host may have many connections => a substantial amount of buffering for the sliding windows is needed
- Buffers are needed at both sender and receiver:
    - At sender side to hold all transmitted but as yet unacknowledged segments, i.e., because these segments may be lost and need to be retransmitted
    - At receiver side a single buffer pool shared by all connections is maintained:
        - On segment reception attempt to dynamically acquire a new buffer:
            - If available the segment is accepted; otherwise, it is discarded.
        - Note: sender is prepared to retransmit segments lost by network => no permanent harm is done by having receiver drop segments but wastage of resources


----


Q: What happens if network layer delivers data faster than application layer removes data from socket buffers?
flow control: receiver controls sender, so sender won’t overflow receiver’s buffer by transmitting too much, too fast


![](image/Pasted%20image%2020241208180849.png)


- TCP receiver “advertises” free buffer space in rwnd field in TCP header
    - receiver 告诉 sender, receiver 中的 buffer 还有多少 free space 
    - RcvBuffer size set via socket options (typical default is 4096 bytes)
    - many operating systems autoadjust RcvBuffer
- sender limits amount of unACKed (“in-flight”) data to received rwnd
- guarantees receive buffer will not overflow

![](image/Pasted%20image%2020241208181001.png)


## 8.1 buffer strategies (adjust their buffer allocations)

- Different buffer strategies trade efficiency / complexity
    - Low-bandwidth bursty traffic: reasonable not to dedicate any buffers, but rather to acquire them dynamically at both ends
    - High-bandwidth traffic: better if receiver dedicates a full window of buffers => allows the data to flow at maximum speed (strategy used by TCP)
- Question: How to organize the buffer pool?

![](image/Pasted%20image%2020241208193928.png)


Sender and receiver need to dynamically adjust their buffer allocations due to traffic pattern changes + changes in number of connections
- Option 1:
    - Sending host requests buffer space at the other end
    - Buffers could be allocated per connection or shared by all connections
- Option 2:
    - Receiver, knowing its buffer situation (but not knowing the offered traffic)  could tell the sender ‘‘I have reserved X buffers for you.’’
    - If number of open connections increases => allocation may be dynamically reduced => support by the protocol needed


Basic idea: decouple buffering from acknowledgements 
- Initially, sender requests a certain number of buffers (e.g. based on its expected needs)
- Receiver grants as many of these as it can afford
- Every time sender transmits a segment, it must decrement its allocation, stopping when allocation reaches zero
- Receiver separately piggybacks both acknowledgements and buffer allocations onto reverse traffic
- TCP uses this scheme: buffer allocations carried in header field called Window size


So far we assumed that the only limit imposed on sender’s data rate is amount of buffer space available in receiver
But this is often not the case: Hosts may be equipped with sufficient memory => lack of buffers is rarely, if ever, a problem, even for wide area connections
But there is another bottleneck: carrying capacity of the network
- Assume adjacent routers can exchange at most x packets/sec and there are k disjoint paths between a pair of hosts
- It is impossible for hosts to exchange more than kx segments/sec
- If sender sends segments too fast => network will become congested, i.e., it will be unable to deliver segments as fast as they are coming in



![](image/Pasted%20image%2020241208194102.png)

# 9 Congestion Control: Desirable Bandwidth Allocation

## 9.1 Question: What are we trying to achieve by running a congestion control algorithm?

Answer:
- We need to specify the state in which a good congestion control algorithm will operate the network
- More than simply avoiding congestion => ==need to find a good allocation of bandwidth to transport entities that are using the network ==
    -  congestion control 的本质是 find a good allocation of bandwidth
- A good allocation:
    1. Delivers good **performance** because it uses all available bandwidth but avoids congestion
    2. It will be **fair** across competing transport entities
    3. It will quickly track changes in traffic demands


- Efficient use of bandwidth gives high goodput, low delay
- Goodput 实际吞吐 (or rate of useful packets arriving at receiver) and delay as a function of the offered load.


Congestion collapse: 拥塞发生的那一刻 , buffer 满了, reveicer 端 不再收到有用的 packet 了 都堵了 
![](image/Pasted%20image%2020241212225749.png)

Onset of Congestion : 拥塞发生的那一刻 , buffer 满了, reveicer 端 不再收到有用的 packet 了 都堵了 , 自然 收到 有效的 packet 的时间就电厂了 
![](image/Pasted%20image%2020241212225903.png)


- Performance begins to degrade at the onset of congestion
- Best performance is obtained if we allocate bandwidth up until the delay starts to climb rapidly
- This point is below the capacity
    - To identify it, Kleinrock (1979) proposed the metric of power:  power = load/delay 
- Note: load with highest power represents an efficient load for transport entity to place on the network


## 9.2 Next question: How to divide bandwidth between different transport senders?

- Possible answer: give all senders an equal fraction of bandwidth
- … but what has this to do with congestion control?
    - Networks often have no strict bandwidth reservation for each flow or connection => e.g., in IP routers all connections are competing for the same bandwidth => here the congestion control mechanism is responsible for allocating bandwidth to the competing connections


## 9.3 Question: What does a fair portion means for flows in a network? 

- Simple case:
    - N flows sharing a single link => each gets 1/N of bandwidth
- More complicated:
    - Flows having different, but overlapping, network paths
    - Example:
        - One flow may cross three links, and the other flows may cross one link
        - Three-link flow consumes more network resources => might be fairer to give it less bandwidth than the one-link flows

=> Tension between fairness and efficiency

### 9.3.1 Max-min fairness

Max-Min Fairness is a resource allocation principle used to fairly distribute bandwidth or other resources among multiple competing users or flows while ensuring that ==the most disadvantaged flows are prioritized.==

- We define notion of fairness that does not depend on length of network path
    - … looks simple but is complicated as different connections will take different paths through network and these paths will may have different capacities
- Max-min fairness
    - Often desired for network usage
    - Gives bandwidth to all flows (no starvation)
    - Gives equal shares of bottleneck
- Fair use gives bandwidth to all flows (no starvation)
- Max-min fairness gives equal shares of bottleneck

- Max-min allocations can be computed given a global knowledge of the network
- Intuitive way to think about them:
    - Rate for all flows starts at zero and is slowly increased
    - When rate reaches a bottleneck for any flow => that flow stops increasing
    - Other flows continue to increase, sharing equally in the available capacity, until they too reach their respective bottlenecks
- Another issue: level over which to consider fairness
    - fairness per host vs. fairness per connection

Advantages of Max-Min Fairness
- **Fair Allocation**: Ensures small demands are met first.
- **No Starvation**: Guarantees every flow gets some resource.
- **Simple Implementation**: Iterative algorithms are straightforward.


4 flows A-D

![](image/Pasted%20image%2020241212232002.png)



---



Key Concepts of Max-Min Fairness

- **Fair Distribution**:
    - Bandwidth is allocated in such a way that increasing the allocation for one flow reduces the allocation for another flow that has an equal or smaller share.
    - 
- **Prioritization**:
    - Smaller flows (those needing less bandwidth) are satisfied first, and leftover bandwidth is distributed to larger flows.
- **No Starvation**:
    - All flows receive some bandwidth, ensuring no flow is completely denied access.
- **Pareto Efficiency**:
    - No flow's allocation can be increased without decreasing the allocation of a flow with a smaller or equal allocation.
    - You cannot increase the allocation of any flow unless you decrease the allocation of another flow with an equal or smaller allocation.
    - This ensures that fairness is preserved, and smaller allocations are not reduced to increase larger ones.
    - 我们必须减少 原来大的 allocation of flow , 来 增加 原本小的 allocation of flow 
    - 我门不可以 减少 小的 allocation of lows, 来增加 原本大的 allocation of flow 

![](image/Pasted%20image%2020241212235717.png)



---

Conclusion
The principle ensures that Max-Min Fairness is:

- **Prioritized**: Smaller or underprivileged flows are protected.
- **Stable**: Changes in allocation require a proportional consideration of fairness across all flows.

Algorithm for Max-Min Fairness in Bandwidth Allocation

- **Sort Flows**:
    - Sort flows in ascending order of their requested bandwidth.
- **Iterative Allocation**:
    - Allocate bandwidth incrementally:
        - Initially, allocate the same amount of bandwidth to all flows.
        - If a flow reaches its requested bandwidth, freeze its allocation, and redistribute the remaining bandwidth among the unfrozen flows.
- **Terminate**:
    - Stop when all bandwidth is allocated or all flows are satisfied.

----


Example1: Max-Min Fairness in Bandwidth

Scenario:
- Total bandwidth: 20 Mbps
- Flows and their requested bandwidth:
    - Flow A: 5 Mbps
    - Flow B: 10 Mbps
    - Flow C: 15 Mbps

Steps:
1. **Initial Allocation**:
    - Divide bandwidth equally among all flows: 203≈6.67\frac{20}{3} \approx 6.67320​≈6.67 Mbps.
    - Flow A is satisfied (it only needs 5 Mbps).
    - Remaining bandwidth: 20−5=1520 - 5 = 1520−5=15 Mbps.
2. **Reallocate Remaining Bandwidth**:
    - Redistribute among the unfrozen flows (B and C): 152=7.5\frac{15}{2} = 7.5215​=7.5 Mbps each.
Result:
- Flow A: 5 Mbps (fully satisfied)
- Flow B: 7.5 Mbps (fully satisfied)
- Flow C: 7.5  Mbps (partially satisfied)


## 9.4 convergence

- Congestion control algorithm should converge quickly to a fair and efficient allocation of bandwidth
- Challenging as network environment is dynamic: connections are always coming and going in a network, and the bandwidth needed by a given connection will vary over time too (bursty traffic like Web browsing) 
- Goal: rapidly converge to ideal operating point + track that point as it changes over time
- Convergence: We want bandwidth levels to converge quickly when traffic change

![](image/Pasted%20image%2020241213000324.png)





# 10 Congestion Control: Regulating the Sending Rate

Sender may need to slow down for different reasons:
- Flow control, when the receiver is not fast enough
- Congestion control, when the network is not fast enough

(a) a fast network feeding a low-capacity receiver => flow control is needed
(b) a slow network feeding a high-capacity receiver => congestion control is needed 
- Our focus is congestion

![](image/Pasted%20image%2020241213000606.png)

---

Different congestion signals the network may use to tell the transport endpoint to slow down (or speed up)
- In explicit, precise design the routers tell sources the rate at which they may send => e.g., XCP (Explicit Congestion Control)
- Explicit, imprecise design: e.g., use of ECN (Explicit Congestion Notification) with TCP => routers set bits on packets that experience congestion to warn the senders to slow down, but they do not tell them how much to slow down
- Other designs have no explicit signal: e.g., FAST TCP measures roundtrip delay and uses that metric as a signal to avoid congestion
- TCP with drop-tail or RED routers: packet loss is inferred and used to signal that the network has become congested => most prevalent in Internet today


Signals of some congestion control protocols
![](image/Pasted%20image%2020241213000841.png)

## 10.1 The AIMD (Additive Increase Multiplicative Decrease) control law

- We consider case of binary congestion feedback
- If two flows increase/decrease their bandwidth in the same way when the network signals free/busy they will not converge to a fair allocation
- ==最终目的是 通过不断地 Additive Increase Multiplicative Decrease, 使得bandwith 的分配到达 optimale point ==
- The AIMD (Additive Increase Multiplicative Decrease) control law does converge to a fair and efficient point!
    - Multiplicative  Increase Additive Decrease 的方式 是到达不了 Optimale point 的, 因为 斜度的问题 
    - start point 随意给出 

![](image/Pasted%20image%2020241213001054.png)


![](image/Pasted%20image%2020241213001321.png)


--- 

## 10.2 TCP 实际中 是怎么控制 sending rate 的 

- TCP implements an AIMD control law to adjust the sending rate and provide congestion control
- But not so easy as rates are measured over some interval and traffic is bursty
- Instead of adjusting the rate directly, TCP adjusts the size of a sliding window
- If window size is W and the round-trip time is RTT, the equivalent rate is W/RTT
- This strategy is easy to combine with flow control, which already uses a window
- Sender paces 为……定速度 packets using ACKs and hence slows down in one RTT if it stops receiving reports that packets are leaving the network





# 11 Congestion_Control


Congestion:
- informally: “too many sources sending too much data too fast for network to handle”
- manifestations:
    - long delays (queueing in router buffers)
    - packet loss (buffer overflow at routers)
- different from flow control!
- a top-10 problem!



## 11.1 Causes/costs of congestion: 


- Objective: mechanism that limits transmissions from sender based on network’s carrying capacity (rather than on receiver’s buffering capacity) 
- Solution: usage of sliding window flow-control scheme in which sender dynamically adjusts window size to match the network’s carrying capacity
- Note: dynamic sliding window can implement both flow and congestion control!  
    - 使用 动态的大小的 silding winodws也可以起到 congestion control 的作用 
- Optimal window size:
    - Network can handle c segments/sec,  
    - Round-trip time (including transmission, propagation, queueing, processing at the receiver, and return of the acknowledgement) is r
    - Sender’s window should be cr 
    - Sender 会一次发射 windows size 大小的 segment, 就是说, 然后 . 距离下次一次sender 发射 segment 到 Receiver 的时间间隔是r,  . 我们预设 receiver 在下次收到 segenent 前, 能处理 掉 cr 个 segement. 所以 Sender’s window should be cr 
- As network capacity available to any given flow varies over time, the window size should be adjusted frequently … to track changes in the carrying capacity

---

- Issue: If transport entities on many machines send too many packets into the network => network will become congested => performance degraded as packets are delayed and lost
- Controlling congestion is combined responsibility of network and transport layers:
    - Congestion occurs at routers => detected at network layer
    - But congestion is ultimately caused by traffic sent into network by transport layer!
- Approach: ==The only effective way to control congestion is for transport protocols to send packets into network more slowly==

Focus:
- Goals of congestion control
- Algorithms used by hosts to ==regulate rate at which they send packets into network==
- Note: Internet relies heavily on transport layer for congestion control => specific algorithms are built into TCP





![](image/Pasted%20image%2020241208181923.png)

### 11.1.1 scenario 1

![](image/Pasted%20image%2020241208181622.png)

### 11.1.2 scenario 2

![](image/Pasted%20image%2020241208181630.png)

![](image/Pasted%20image%2020241208181706.png)

![](image/Pasted%20image%2020241208181713.png)

![](image/Pasted%20image%2020241208181723.png)

![](image/Pasted%20image%2020241208181743.png)

“costs” of congestion:
- more work (retransmission) for given receiver throughput
- unneeded retransmissions: link carries multiple copies of a packet
    - decreasing maximum achievable throughput

### 11.1.3 scenario 3

![](image/Pasted%20image%2020241208181837.png)


![](image/Pasted%20image%2020241208181848.png)



## 11.2 Approaches towards congestion control


![](image/Pasted%20image%2020241208181957.png)

![](image/Pasted%20image%2020241208182223.png)




## 11.3 AIMD Additive Increase Multiplicative Decrease 

approach: senders can increase sending rate until packet loss (congestion) occurs, then decrease sending rate on loss event


![](image/Pasted%20image%2020241101201639.png)


Multiplicative decrease detail: sending rate is
- Cut in half on loss detected by triple duplicate ACK (TCP Reno)
- Cut to 1 MSS (maximum segment size) when loss detected by timeout (TCP Tahoe)

## 11.4 TCP CUBIC

![](image/Pasted%20image%2020241101201911.png)

increase W as a function of the cube of the distance between current time and K

![](image/Pasted%20image%2020241101202104.png)


# 12 Parallelizing TCP Connections
 parallel TCP connections 就是 在一个bandwidth 中 , 固定的 两方可以同时开 多个 tcp connection , 而不是只能那个开一个 


Assume a large file transfer from a server to a client. The server splits the file into n > 1 pieces and sends the pieces in n parallel TCP connections to the host.
(a) What is the advantage of using parallel connections for the server, rather than sending the whole file in a single TCP connection?
The server can transmit more data over time in total 
the sum of all window sizes of the connections

If one connection experiences packet loss and decreases its window size, the others are not affected 


(b) What is the disadvantage of using parallel connections for the server, rather than sending the whole file in a single TCP connection?

bandwidth 大小是固定的
 在一个bandwidth 中, 固定的 两方可以同时开 多个 tcp connection , , 则会占用 很多的 资源 in bandwidth. 这样 别的 server 之间的通信 能用的 资源就少了 .  
 因为  默认 每个 tcp connection 占用的贷款是固定的 , 一个 bandwidth  中能容纳的 tcp connection 是有限的



(c) Assume that there are other TCP connections in the network, which are not between our server and client. How does using parallel connections between our server and client impact the other TCP connections?

- TCP shares resource fairly 
- The available bandwidth is divided among all connections
- Multiple connections means that the server has more bandwidth available in total 
- if one user or server is using multiple parallel connections , it may dominate the available bandwidth , leaving less for others. This violate the principle of fairness in TCP congestion control 
- This means that the band width allocation between the hosts is no longer fair 





