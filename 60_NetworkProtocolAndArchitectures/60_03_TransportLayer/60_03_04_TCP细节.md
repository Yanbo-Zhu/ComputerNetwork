# 1 传输控制协议TCP简介

•面向连接的、可靠的、基于字节流的传输层通信协议；
•将应用的数据分割成报文段并发送给目标节点的TCP层；
•数据包都有序号Sequence，对方收到则发送ACK确认，未收到则重传；
•使用检验和来检验数据传输过程中是否有误；

- point-to-point:
    - one sender, one receiver
- reliable, in-order byte steam:
    - no “message boundaries"
- full duplex data:
    - bi-directional data flow in same connection
- MSS: maximum segment size
- cumulative ACKs
- pipelining:
    - TCP congestion and flow control set window size
- connection-oriented:
    - handshaking (exchange of control messages) initializes sender, receiver state before data exchange 
- flow controlled:
    - sender will not overwhelm receiver

# 2 TCP和UDP

TCP ist connection oriented reliable transport 
UDP ist connectless transport 


![](../60_01_Intro/image/Pasted%20image%2020241021072630.png)



# 3 Error Control, Congestion Control, and Flow Control

- Objective: management of connections while they are in use
- Key issues:
    - Error control – ensuring that data is delivered with desired level of reliability, e.g. all of the data is delivered without any errors
    - Congestion control – avoid overload situation within the network
    - Flow control – keeping a fast transmitter from overrunning a slow receiver
Note: These issues are also addressed in data link layer (not covered in this lecture)


- Foundation for `error control `is a sliding window with checksums and retransmissions
- `Flow control` manages buffering at sender/receiver
    - Issue is that data goes to/from the network and applications at different times
    - Window tells sender available buffering at receiver
    - Makes a variable-size sliding window
- `Congestion control` limits transmissions from sender based on network’s carrying capacity



# 4 Error Control (checksum and retransmissions)

- Difference in function: e.g., error detection
    - Link layer (LL) checksum protects a frame while it crosses a single link
    - Transport layer checksum protects a segment while it crosses an entire network path
        - Note: packets may get corrupted inside a router
    - End-to-end argument: transport layer check that runs end-to-end is essential for correctness => link layer checks are not essential but improve performance (a corrupted packet will not be sent along the entire path  unnecessarily)

![](image/Pasted%20image%2020241208192900.png)


## 4.1 Difference in degree: e.g., retransmissions and the sliding window protocol

- Most wireless links (except satellite links) can have only a single frame outstanding from the sender at a time, i.e. bandwidth-delay product is too small to store even a whole frame => small window size is sufficient for good performance
- E.g., 802.11 uses a stop-and-wait protocol, i.e., (re-) transmitting each frame and waiting for the ACKed before moving to next frame
    - Larger window size would add complexity without improving performance
- For wired/optical fiber links (e.g., (switched) Ethernet or ISP backbones) the error-rate is low enough that link-layer retransmissions can be omitted => end-to-end retransmissions will repair residual frame loss

But many transport layer (e.g., TCP) connections have a large bandwidth-delay product that is much larger than a single segment
Example:
- Connection sending data across the U.S. at 1 Mbps with a round-trip time of 100 ms
- Here 200 Kbit of data will be stored at receiver in the time it takes to send a segment and receive the ACK
- For such situations, a large sliding window must be used! 
- Note: stop-and-wait will kill performance, e.g. in our example it would limit performance to just 5 segments/sec no matter how fast the network really is!



# 5 round trip time, timeout

Q: how to set TCP timeout value?
- longer than RTT, but RTT varies!
- too short: premature timeout, unnecessary retransmissions
- too long: slow reaction to segment loss

Q: how to estimate RTT?
- SampleRTT:measured time from segment transmission until ACK receipt
    - ignore retransmissions
- SampleRTT will vary, want estimated RTT “smoother”
    - average several recent measurements, not just current SampleRTT

![](image/Pasted%20image%2020241208180348.png)


![](image/Pasted%20image%2020241208180355.png)

# 6 TCP Sender (simplified)

event: data received from application
- create segment with seq 
- seq # is byte-stream number of first data byte in segment
- start timer if not already running
    - think of timer as for oldest unACKed segment
    - expiration interval: TimeOutInterval

event: timeout
- retransmit segment that caused timeout
- restart timer

event: ACK received
- if ACK acknowledges previously unACKed segments
- update what is known to be ACKed
- start timer if there are still unACKed segments

![](image/Pasted%20image%2020241208183019.png)


## 6.1 TCP: retransmission scenarios

![](image/Pasted%20image%2020241208180626.png)


![](image/Pasted%20image%2020241208180632.png)



![](image/Pasted%20image%2020241208180715.png)


# 7 Flow Control 

brief preview from data link layer

![](image/Pasted%20image%2020241207120429.png)

1. Frame carries an error-detecting code (e.g., CRC) to check if information was correctly received
2. Frame carries a sequence number to identify itself and is retransmitted by the sender until it receives an ACK of successful receipt from the receiver => ARQ (Automatic Repeat reQuest)
3. There is a maximum number of frames M that the sender will allow to be outstanding at any time, pausing if the receiver is not acknowledging frames quickly enough:
    1. If M=1: protocol is called stop-and-wait.
    2. M>1 enables pipelining and improved performance on long, fast links
4. Sliding window protocol combines these features + used to support bidirectional data transfer


## 7.1 flow control protocols
This description relates to **flow control protocols** used in computer networks for reliable data transmission. Here's a breakdown of the key points:
1. **Maximum Number of Outstanding Frames (M):**
    - This is the maximum number of data frames a sender can transmit without waiting for an acknowledgment (ACK) from the receiver.
    - It determines the level of **pipelining**, which allows for more efficient use of the communication channel.

--- 

Protocols Based on `M`:
Stop-and-Wait Protocol (M = 1):
- The sender transmits one frame and waits for an acknowledgment before sending the next frame.
- **Advantages:**
    - Simple to implement.
    - Works well on low-speed or low-latency networks.
- **Disadvantages:**
    - Inefficient on high-latency or high-bandwidth links because the sender remains idle while waiting for an acknowledgment.


Sliding Window Protocol (M > 1):
- The sender can transmit up to `M` frames without waiting for an acknowledgment.
- **Pipelining:** Allows multiple frames to be "in transit" simultaneously, improving performance by utilizing the bandwidth-delay product of the link.
- The receiver acknowledges frames and informs the sender of the next expected frame, enabling the sender to slide the "window" forward.
- **Advantages:**
    - Significantly higher throughput on high-latency or high-bandwidth links.
- **Disadvantages:**
    - More complex to implement than stop-and-wait.
    - Requires mechanisms to handle lost or out-of-order frames.


Key Concepts:
1. **Acknowledgment (ACK):**
    - A signal sent by the receiver to confirm successful receipt of a frame.
2. **Flow Control:**
    - Mechanism to prevent the sender from overwhelming the receiver (e.g., by sending too many frames at once).
3. **Retransmission:**
    - If frames or ACKs are lost, the sender may retransmit the affected frames based on timers or duplicate ACKs.

Use Cases:
- **Stop-and-Wait:** Suitable for short, low-latency links like local area networks (LANs).
- **Sliding Window:** Essential for long-distance, high-speed networks like satellite communications or wide area networks (WANs).



## 7.2 Sliding Window

- Allows continuous transmission of packets (called frames in data link)
- Continuous transmission means that the sender is allowed to send continuously without waiting for every packet to be acknowledged

![](image/Pasted%20image%2020241207121250.png)


# 8 TCP flow control (Buffering)

- As transport protocols generally use larger sliding windows we have to look at the issue of buffering data more carefully
    - … a host may have many connections => a substantial amount of buffering for the sliding windows is needed
- Buffers are needed at both sender and receiver:
    - At sender side to hold all transmitted but as yet unacknowledged segments, i.e., because these segments may be lost and need to be retransmitted
    - At receiver side a single buffer pool shared by all connections is maintained:
        - On segment reception attempt to dynamically acquire a new buffer:
            - If available the segment is accepted; otherwise, it is discarded.
        - Note: sender is prepared to retransmit segments lost by network => no permanent harm is done by having receiver drop segments but wastage of resources


----


Q: What happens if network layer delivers data faster than application layer removes data from socket buffers?
flow control: receiver controls sender, so sender won’t overflow receiver’s buffer by transmitting too much, too fast


![](image/Pasted%20image%2020241208180849.png)


- TCP receiver “advertises” free buffer space in rwnd field in TCP header
    - receiver 告诉 sender, receiver 中的 buffer 还有多少 free space 
    - RcvBuffer size set via socket options (typical default is 4096 bytes)
    - many operating systems autoadjust RcvBuffer
- sender limits amount of unACKed (“in-flight”) data to received rwnd
- guarantees receive buffer will not overflow

![](image/Pasted%20image%2020241208181001.png)


## 8.1 buffer strategies (adjust their buffer allocations)

- Different buffer strategies trade efficiency / complexity
    - Low-bandwidth bursty traffic: reasonable not to dedicate any buffers, but rather to acquire them dynamically at both ends
    - High-bandwidth traffic: better if receiver dedicates a full window of buffers => allows the data to flow at maximum speed (strategy used by TCP)
- Question: How to organize the buffer pool?

![](image/Pasted%20image%2020241208193928.png)


Sender and receiver need to dynamically adjust their buffer allocations due to traffic pattern changes + changes in number of connections
- Option 1:
    - Sending host requests buffer space at the other end
    - Buffers could be allocated per connection or shared by all connections
- Option 2:
    - Receiver, knowing its buffer situation (but not knowing the offered traffic)  could tell the sender ‘‘I have reserved X buffers for you.’’
    - If number of open connections increases => allocation may be dynamically reduced => support by the protocol needed


Basic idea: decouple buffering from acknowledgements 
- Initially, sender requests a certain number of buffers (e.g. based on its expected needs)
- Receiver grants as many of these as it can afford
- Every time sender transmits a segment, it must decrement its allocation, stopping when allocation reaches zero
- Receiver separately piggybacks both acknowledgements and buffer allocations onto reverse traffic
- TCP uses this scheme: buffer allocations carried in header field called Window size


So far we assumed that the only limit imposed on sender’s data rate is amount of buffer space available in receiver
But this is often not the case: Hosts may be equipped with sufficient memory => lack of buffers is rarely, if ever, a problem, even for wide area connections
But there is another bottleneck: carrying capacity of the network
- Assume adjacent routers can exchange at most x packets/sec and there are k disjoint paths between a pair of hosts
- It is impossible for hosts to exchange more than kx segments/sec
- If sender sends segments too fast => network will become congested, i.e., it will be unable to deliver segments as fast as they are coming in



![](image/Pasted%20image%2020241208194102.png)



# 9 TCP_Congestion_Control


Congestion:
- informally: “too many sources sending too much data too fast for network to handle”
- manifestations:
    - long delays (queueing in router buffers)
    - packet loss (buffer overflow at routers)
- different from flow control!
- a top-10 problem!


- Objective: mechanism that limits transmissions from sender based on network’s carrying capacity (rather than on receiver’s buffering capacity) 
- Solution: usage of sliding window flow-control scheme in which sender dynamically adjusts window size to match the network’s carrying capacity
- Note: dynamic sliding window can implement both flow and congestion control!
- Optimal window size:
    - Network can handle c segments/sec,
    - Round-trip time (including transmission, propagation, queueing, processing at the receiver, and return of the acknowledgement) is r
    - Sender’s window should be cr
- As network capacity available to any given flow varies over time, the window size should be adjusted frequently … to track changes in the carrying capacity

- Issue: If transport entities on many machines send too many packets into the network => network will become congested => performance degraded as packets are delayed and lost
- Controlling congestion is combined responsibility of network and transport layers:
    - Congestion occurs at routers => detected at network layer
    - But congestion is ultimately caused by traffic sent into network by transport layer!
- Approach: The only effective way to control congestion is for transport protocols to send packets into network more slowly

Focus:
- Goals of congestion control
- Algorithms used by hosts to regulate rate at which they send packets into network
- Note: Internet relies heavily on transport layer for congestion control => specific algorithms are built into TCP





# 10 Desirable Bandwidth Allocation



## 10.1 Causes/costs of congestion: 


![](image/Pasted%20image%2020241208181923.png)

### 10.1.1 scenario 1

![](image/Pasted%20image%2020241208181622.png)

### 10.1.2 scenario 2

![](image/Pasted%20image%2020241208181630.png)

![](image/Pasted%20image%2020241208181706.png)

![](image/Pasted%20image%2020241208181713.png)

![](image/Pasted%20image%2020241208181723.png)

![](image/Pasted%20image%2020241208181743.png)

“costs” of congestion:
- more work (retransmission) for given receiver throughput
- unneeded retransmissions: link carries multiple copies of a packet
    - decreasing maximum achievable throughput

### 10.1.3 scenario 3

![](image/Pasted%20image%2020241208181837.png)


![](image/Pasted%20image%2020241208181848.png)



## 10.2 Approaches towards congestion control


![](image/Pasted%20image%2020241208181957.png)

![](image/Pasted%20image%2020241208182223.png)




## 10.3 AIMD Additive Increase Multiplicative Decrease 

approach: senders can increase sending rate until packet loss (congestion) occurs, then decrease sending rate on loss event


![](image/Pasted%20image%2020241101201639.png)


Multiplicative decrease detail: sending rate is
- Cut in half on loss detected by triple duplicate ACK (TCP Reno)
- Cut to 1 MSS (maximum segment size) when loss detected by timeout (TCP Tahoe)

## 10.4 TCP CUBIC

![](image/Pasted%20image%2020241101201911.png)

increase W as a function of the cube of the distance between current time and K

![](image/Pasted%20image%2020241101202104.png)


# 11 Parallelizing TCP Connections
 parallel TCP connections 就是 在一个bandwidth 中 , 固定的 两方可以同时开 多个 tcp connection , 而不是只能那个开一个 


Assume a large file transfer from a server to a client. The server splits the file into n > 1 pieces and sends the pieces in n parallel TCP connections to the host.
(a) What is the advantage of using parallel connections for the server, rather than sending the whole file in a single TCP connection?
The server can transmit more data over time in total 
the sum of all window sizes of the connections

If one connection experiences packet loss and decreases its window size, the others are not affected 


(b) What is the disadvantage of using parallel connections for the server, rather than sending the whole file in a single TCP connection?

bandwidth 大小是固定的
 在一个bandwidth 中, 固定的 两方可以同时开 多个 tcp connection , , 则会占用 很多的 资源 in bandwidth. 这样 别的 server 之间的通信 能用的 资源就少了 .  
 因为  默认 每个 tcp connection 占用的贷款是固定的 , 一个 bandwidth  中能容纳的 tcp connection 是有限的



(c) Assume that there are other TCP connections in the network, which are not between our server and client. How does using parallel connections between our server and client impact the other TCP connections?

- TCP shares resource fairly 
- The available bandwidth is divided among all connections
- Multiple connections means that the server has more bandwidth available in total 
- if one user or server is using multiple parallel connections , it may dominate the available bandwidth , leaving less for others. This violate the principle of fairness in TCP congestion control 
- This means that the band width allocation between the hosts is no longer fair 





