# 1 传输控制协议TCP简介

TCP (Transmission Control Protocol) was specifically designed to provide a reliable end-to-end byte stream over an unreliable internetwork

•面向连接的、可靠的、基于字节流的传输层通信协议；
•将应用的数据分割成报文段并发送给目标节点的TCP层；
•数据包都有序号Sequence，对方收到则发送ACK确认，未收到则重传；
•使用检验和来检验数据传输过程中是否有误；


# 2 TCP Main Features

- Bi-directional byte stream between two endpoints
- Connection-oriented
    - connection is established in order to transmit data
- Offers reliable in-order delivery
    - every packet is delivered at least once (no loss)
    - every packet is delivered at most once (no duplicates)
    - packets have data integrity (no bit errors)
    - packets are also delivered in the order they were transmitted to guarantee inorder reliable delivery
- Tuned to avoid congestion
    - implements congestion avoidance and control
- Mechanisms used in Reliable Transport
    - Packets can get corrupted: CRC or checksum to detect, retransmission to recover
- Packets can get lost: Acknowledgement + timeout to detect, retransmission to recover
- Packets can get re-ordered:  Sequence number to detect, receiver buffer to re-order

- **_Ende-zu-Ende-Transportprotokoll_** im Internet
- **_Zuverlässig_**, **_verbindungsorientiert_**, **_paketvermittelt_**
- **_Bidirektional_**, d.h. Übertragung in beide Richtungen
- **_Flusskontrolle_** vermeidet Überlastung des Empfängers
- **_Staukontrolle_** drosselt die Übertragung bei (angeblicher) Überlastung des Netzes
- **_Fehlerkontrolle_** ermöglicht Wiederübertragung bei fehlerhaften oder verlorenen Segmenten 
- Sender: Unterteilt den eingehenden Datenstrom auf mehrere Segmente und übergibt sie der Internetschicht
- Empfänger: Setzt die eingehenden Segmente in der ursprünglichen Reihenfolge zu einem Ausgabestrom zusammen und übergibt ihn der Anwendungsschicht


- point-to-point:
    - one sender, one receiver
- reliable, in-order byte steam:
    - no “message boundaries"
- full duplex data:
    - bi-directional data flow in same connection
- MSS: maximum segment size
- cumulative ACKs
- pipelining:
    - TCP congestion and flow control set window size
- connection-oriented:
    - handshaking (exchange of control messages) initializes sender, receiver state before data exchange 
- flow controlled:
    - sender will not overwhelm receiver


- Each machine has a TCP transport entity (mostly part of OS kernel)
    - It manages TCP streams & interfaces to IP layer
    - It accepts user data streams from processes, breaks them up into pieces (< 64 KB; in practice: 1460 B), and sends each piece as a separate IP datagram
    - When datagrams containing TCP data arrive at receiver machine, they are given to TCP entity, which reconstructs original byte streams
- Challenging as:
    - IP layer gives no guarantee that datagrams will be delivered properly, nor any indication of how fast datagrams may be sent
    - TCP need to send datagrams fast enough to make use of capacity but not cause congestion, and to time out and retransmit any datagrams that are not delivered
    - TCP needs to reorder datagrams that arrive in wrong order

# 3 The TCP Service Model

TCP provides applications with a reliable byte stream between processes; it is the workhorse of the Internet
Popular servers run on well-known ports (see www.iana.org)

![](image/Pasted%20image%2020250107231830.png)


Applications using TCP see only the byte stream [right] and not the segments [left] sent as separate IP packets
![](image/Pasted%20image%2020250107231850.png)


internally stream of bytes is sent inside TCP segments
![](image/Pasted%20image%2020250107231902.png)






# 4 TCP和UDP

TCP ist connection oriented reliable transport 
UDP ist connectless transport 


![](../60_01_Intro/image/Pasted%20image%2020241021072630.png)



# 5 Error Control, Congestion Control, and Flow Control

- Objective: management of connections while they are in use
- Key issues:
    - Error control – ensuring that data is delivered with desired level of reliability, e.g. all of the data is delivered without any errors
    - Congestion control – avoid overload situation within the network
    - Flow control – keeping a fast transmitter from overrunning a slow receiver
Note: These issues are also addressed in data link layer (not covered in this lecture)


- Foundation for `error control `is a sliding window with checksums and retransmissions
- `Flow control` manages buffering at sender/receiver
    - Issue is that data goes to/from the network and applications at different times
    - Window tells sender available buffering at receiver
    - Makes a variable-size sliding window
- `Congestion control` limits transmissions from sender based on network’s carrying capacity



# 6 Error Control (checksum and retransmissions)

- Difference in function: e.g., error detection
    - Link layer (LL) checksum protects a frame while it crosses a single link
    - Transport layer checksum protects a segment while it crosses an entire network path
        - Note: packets may get corrupted inside a router
    - End-to-end argument: transport layer check that runs end-to-end is essential for correctness => link layer checks are not essential but improve performance (a corrupted packet will not be sent along the entire path  unnecessarily)

![](image/Pasted%20image%2020241208192900.png)


## 6.1 Difference in degree: e.g., retransmissions and the sliding window protocol

- Most wireless links (except satellite links) can have only a single frame outstanding from the sender at a time, i.e. bandwidth-delay product is too small to store even a whole frame => small window size is sufficient for good performance
- E.g., 802.11 uses a stop-and-wait protocol, i.e., (re-) transmitting each frame and waiting for the ACKed before moving to next frame
    - Larger window size would add complexity without improving performance
- For wired/optical fiber links (e.g., (switched) Ethernet or ISP backbones) the error-rate is low enough that link-layer retransmissions can be omitted => end-to-end retransmissions will repair residual frame loss

But many transport layer (e.g., TCP) connections have a large bandwidth-delay product that is much larger than a single segment
Example:
- Connection sending data across the U.S. at 1 Mbps with a round-trip time of 100 ms
- Here 200 Kbit of data will be stored at receiver in the time it takes to send a segment and receive the ACK
- For such situations, a large sliding window must be used! 
- Note: stop-and-wait will kill performance, e.g. in our example it would limit performance to just 5 segments/sec no matter how fast the network really is!



# 7 Round trip time, timeout

Q: how to set TCP timeout value?
- longer than RTT, but RTT varies!
- too short: premature timeout, unnecessary retransmissions
- too long: slow reaction to segment loss

Q: how to estimate RTT?
- SampleRTT:measured time from segment transmission until ACK receipt
    - ignore retransmissions
- SampleRTT will vary, want estimated RTT “smoother”
    - average several recent measurements, not just current SampleRTT

![](image/Pasted%20image%2020241208180348.png)


![](image/Pasted%20image%2020241208180355.png)

# 8 TCP Sender (simplified)

event: data received from application
- create segment with seq 
- seq # is byte-stream number of first data byte in segment
- start timer if not already running
    - think of timer as for oldest unACKed segment
    - expiration interval: TimeOutInterval

event: timeout
- retransmit segment that caused timeout
- restart timer

event: ACK received
- if ACK acknowledges previously unACKed segments
- update what is known to be ACKed
- start timer if there are still unACKed segments

![](image/Pasted%20image%2020241208183019.png)


## 8.1 TCP: retransmission scenarios

![](image/Pasted%20image%2020241208180626.png)


![](image/Pasted%20image%2020241208180632.png)



![](image/Pasted%20image%2020241208180715.png)


---
Timer threshold 设的太长, server 就会一直等待. 太短, retransmission 就会太频繁 

TCP estimates retransmit timer from segment RTTs
- Tracks both average (EWMA) and variance (for Internet case)
    - SRTT = α SRTT + (1 − α) RTT (SRTT – smoothed RTT)
    - RTTVAR = β RTTVAR + (1 − β) |SRTT − RTT |
- Timeout is set to average plus 4 x variance
    - RTO = SRTT + 4 × RTTVAR

![](image/Pasted%20image%2020250107234910.png)



# 9 Flow Control 

brief preview from data link layer

![](image/Pasted%20image%2020241207120429.png)

1. Frame carries an error-detecting code (e.g., CRC) to check if information was correctly received
2. Frame carries a sequence number to identify itself and is retransmitted by the sender until it receives an ACK of successful receipt from the receiver => ARQ (Automatic Repeat reQuest)
3. There is a maximum number of frames M that the sender will allow to be outstanding at any time, pausing if the receiver is not acknowledging frames quickly enough:
    1. If M=1: protocol is called stop-and-wait.
    2. M>1 enables pipelining and improved performance on long, fast links
4. Sliding window protocol combines these features + used to support bidirectional data transfer


## 9.1 flow control protocols
This description relates to **flow control protocols** used in computer networks for reliable data transmission. Here's a breakdown of the key points:
1. **Maximum Number of Outstanding Frames (M):**
    - This is the maximum number of data frames a sender can transmit without waiting for an acknowledgment (ACK) from the receiver.
    - It determines the level of **pipelining**, which allows for more efficient use of the communication channel.

--- 

Protocols Based on `M`:
Stop-and-Wait Protocol (M = 1):
- The sender transmits one frame and waits for an acknowledgment before sending the next frame.
- **Advantages:**
    - Simple to implement.
    - Works well on low-speed or low-latency networks.
- **Disadvantages:**
    - Inefficient on high-latency or high-bandwidth links because the sender remains idle while waiting for an acknowledgment.


Sliding Window Protocol (M > 1):
- The sender can transmit up to `M` frames without waiting for an acknowledgment.
- **Pipelining:** Allows multiple frames to be "in transit" simultaneously, improving performance by utilizing the bandwidth-delay product of the link.
- The receiver acknowledges frames and informs the sender of the next expected frame, enabling the sender to slide the "window" forward.
- **Advantages:**
    - Significantly higher throughput on high-latency or high-bandwidth links.
- **Disadvantages:**
    - More complex to implement than stop-and-wait.
    - Requires mechanisms to handle lost or out-of-order frames.


Key Concepts:
1. **Acknowledgment (ACK):**
    - A signal sent by the receiver to confirm successful receipt of a frame.
2. **Flow Control:**
    - Mechanism to prevent the sender from overwhelming the receiver (e.g., by sending too many frames at once).
3. **Retransmission:**
    - If frames or ACKs are lost, the sender may retransmit the affected frames based on timers or duplicate ACKs.

Use Cases:
- **Stop-and-Wait:** Suitable for short, low-latency links like local area networks (LANs).
- **Sliding Window:** Essential for long-distance, high-speed networks like satellite communications or wide area networks (WANs).



## 9.2 Sliding Window

- Allows continuous transmission of packets (called frames in data link)
- Continuous transmission means that the sender is allowed to send continuously without waiting for every packet to be acknowledged

![](image/Pasted%20image%2020241207121250.png)


- To keep the pipe full
    - Send multiple packets without waiting for first to be acked
    - Bytes in flight = window
- TCP adds flow control to the sliding window as before
    - ACK + WIN is the sender’s limit
![](image/Pasted%20image%2020250107234300.png)


Need to add special cases to avoid unwanted behavior
- E.g., silly window syndrome
- Receiver application reads single bytes, so sender always sends one byte segments
![](image/Pasted%20image%2020250107234342.png)

# 10 TCP flow control (Buffering)

- As transport protocols generally use larger sliding windows we have to look at the issue of buffering data more carefully
    - … a host may have many connections => a substantial amount of buffering for the sliding windows is needed
- Buffers are needed at both sender and receiver:
    - At sender side to hold all transmitted but as yet unacknowledged segments, i.e., because these segments may be lost and need to be retransmitted
    - At receiver side a single buffer pool shared by all connections is maintained:
        - On segment reception attempt to dynamically acquire a new buffer:
            - If available the segment is accepted; otherwise, it is discarded.
        - Note: sender is prepared to retransmit segments lost by network => no permanent harm is done by having receiver drop segments but wastage of resources


----


Q: What happens if network layer delivers data faster than application layer removes data from socket buffers?
flow control: receiver controls sender, so sender won’t overflow receiver’s buffer by transmitting too much, too fast


![](image/Pasted%20image%2020241208180849.png)


- TCP receiver “advertises” free buffer space in rwnd field in TCP header
    - receiver 告诉 sender, receiver 中的 buffer 还有多少 free space 
    - RcvBuffer size set via socket options (typical default is 4096 bytes)
    - many operating systems autoadjust RcvBuffer
- sender limits amount of unACKed (“in-flight”) data to received rwnd
- guarantees receive buffer will not overflow

![](image/Pasted%20image%2020241208181001.png)


## 10.1 buffer strategies (adjust their buffer allocations)

- Different buffer strategies trade efficiency / complexity
    - Low-bandwidth bursty traffic: reasonable not to dedicate any buffers, but rather to acquire them dynamically at both ends
    - High-bandwidth traffic: better if receiver dedicates a full window of buffers => allows the data to flow at maximum speed (strategy used by TCP)
- Question: How to organize the buffer pool?

![](image/Pasted%20image%2020241208193928.png)


Sender and receiver need to dynamically adjust their buffer allocations due to traffic pattern changes + changes in number of connections
- Option 1:
    - Sending host requests buffer space at the other end
    - Buffers could be allocated per connection or shared by all connections
- Option 2:
    - Receiver, knowing its buffer situation (but not knowing the offered traffic)  could tell the sender ‘‘I have reserved X buffers for you.’’
    - If number of open connections increases => allocation may be dynamically reduced => support by the protocol needed


Basic idea: decouple buffering from acknowledgements 
- Initially, sender requests a certain number of buffers (e.g. based on its expected needs)
- Receiver grants as many of these as it can afford
- Every time sender transmits a segment, it must decrement its allocation, stopping when allocation reaches zero
- Receiver separately piggybacks both acknowledgements and buffer allocations onto reverse traffic
- TCP uses this scheme: buffer allocations carried in header field called Window size


So far we assumed that the only limit imposed on sender’s data rate is amount of buffer space available in receiver
But this is often not the case: Hosts may be equipped with sufficient memory => lack of buffers is rarely, if ever, a problem, even for wide area connections
But there is another bottleneck: carrying capacity of the network
- Assume adjacent routers can exchange at most x packets/sec and there are k disjoint paths between a pair of hosts
- It is impossible for hosts to exchange more than kx segments/sec
- If sender sends segments too fast => network will become congested, i.e., it will be unable to deliver segments as fast as they are coming in



![](image/Pasted%20image%2020241208194102.png)


# 11 Parallelizing TCP Connections
 parallel TCP connections 就是 在一个bandwidth 中 , 固定的 两方可以同时开 多个 tcp connection , 而不是只能那个开一个 


Assume a large file transfer from a server to a client. The server splits the file into n > 1 pieces and sends the pieces in n parallel TCP connections to the host.
(a) What is the advantage of using parallel connections for the server, rather than sending the whole file in a single TCP connection?
The server can transmit more data over time in total 
the sum of all window sizes of the connections

If one connection experiences packet loss and decreases its window size, the others are not affected 


(b) What is the disadvantage of using parallel connections for the server, rather than sending the whole file in a single TCP connection?

bandwidth 大小是固定的
 在一个bandwidth 中, 固定的 两方可以同时开 多个 tcp connection , , 则会占用 很多的 资源 in bandwidth. 这样 别的 server 之间的通信 能用的 资源就少了 .  
 因为  默认 每个 tcp connection 占用的贷款是固定的 , 一个 bandwidth  中能容纳的 tcp connection 是有限的



(c) Assume that there are other TCP connections in the network, which are not between our server and client. How does using parallel connections between our server and client impact the other TCP connections?

- TCP shares resource fairly 
- The available bandwidth is divided among all connections
- Multiple connections means that the server has more bandwidth available in total 
- if one user or server is using multiple parallel connections , it may dominate the available bandwidth , leaving less for others. This violate the principle of fairness in TCP congestion control 
- This means that the band width allocation between the hosts is no longer fair 



# 12 Performance Issues

Many strategies for getting good performance have been learned over time
- Performance problems
- Measuring network performance
- Host design for fast networks
- Fast segment processing
- Header compression
- Protocols for “long fat” networks

## 12.1 Header Compression

Overhead can be very large for small packets
- 40 bytes of header for RTP/UDP/IP VoIP packet
- Problematic for slow links, especially wireless

Header compression mitigates this problem
- Runs between Link and Network layer
- Omits fields that don’t change or change predictably
    - −40 byte TCP/IP header => 3 bytes of information
- Gives simple high-layer headers and efficient links


## 12.2 Protocols for “Long Fat” Networks

- Networks with high bandwidth (“Fat”) and high delay (“Long”) can store much information inside the network
    - Requires protocols with ample 充足的，充裕的 buffering and few RTTs, rather than reducing the bits on the wire
    - ![](image/Pasted%20image%2020250108000314.png)
- You can buy more bandwidth but not lower delay
    - Need to shift ends (e.g., into cloud) to lower further
    - ![](image/Pasted%20image%2020250108000420.png)